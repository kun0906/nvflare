Details of Reproducing Simulation Results of the Three ProjectsAll three projects are located in the “nvflare/auto_labeling/” directory, named IoT, ICONIP, and DROPOUT.  All the datasets (and the corresponding preprocessing scripts) are stored in the “nvflare/auto_labeling/data” directory. All temporary, in-development, and legacy code is stored in the “dev” directory and can be safely ignored. 1. IOTThis project implements the Refined Krum, Krum, and Mean aggregation methods. The project mainly includes: * ragg folder : o robust_aggregation.py: Implements different aggregation methods, including Mean, Media, Krum, and the Refined Krum (named adaptive_krum in the code)o change_point_detection.py: Implements the change point detection method o base.py: Defines the base class for neural network models. o utils.py: Contains utility functions, e.g., timer and Dirichlet functions * replication_neurips folder: Code for the classical Krum papero ragg_random_spambase_gaussian.py: Implements the gaussian attacko ragg_random_spambase_omniscient.py: Implements the omniscient attack   Code for MNIST:o ragg_model_random_noise.py: Code for Random Noise Attack o ragg_model_large_value.py: Code for Large Value Attack o ragg_label_flipping.py: Code for Label Flipping Attack   Code for SENTIMENT140:o ragg_random_noise_model_sentiment140.py: Code for Random Noise Attack o ragg_model_large_value_sentiment140.py: Code for Large Value Attack o ragg_label_flipping_sentiment140.py: Code for Label Flipping Attack The conda environment created for this project:       nvflare-3.10      Python Version: 	      Python 3.10.14Required Libraries:      IoT_Requirements.txt        Details of all libraries included in this conda environment:       IoT_All_Libraries.txt       Experimental Instructions:	# Change directory to IoT folder 	cd IoT# Run the following Python script to submit jobs to SuperPOD for three cases on MNIST and SENTIMENT140: Random Noise Attack, Large Outlier Attack, and Label Flipping Attack.      module load conda      conda activate nvflare-3.10      # This is a lightweight script for submitting jobs using sbatch	python3 sbatch_all.py # Submit jobs to SuperPOD for two cases on SPAMBASE: Gaussian and Omniscient Attacks.	sbatch replication_neurips/sbatch_robust_neurips.sh	# Submit jobs for two extra cases (appeared in the Krum Paper): No Byzantine Clients and 33% Byzantine Clients. Note that these results were not included in the IoT Paper.	sbatch sbatch_robust_neurips.sh# Run the following code to extract results from the log files and save them to an Excel (.xlsx) file. Before running the code, the JOBID should be replaced, which can be found in the log file name. python plot_robust_aggregation.py 	2. ICONIP: This project implements the Average-rKrum (ArKrum), Refined Krum (rKrum), multi-Krum (mKrum), Krum, and Mean aggregation methods. The project mainly includes: * ragg folder : o robust_aggregation.py: Implements different aggregation methods, including Mean, Media, Krum, and the Refined Krum (named adaptive_krum in the code). Here mKrum is implemented based on the Krum function, and ArKrum is implemented based on the Refined Krum function o change_point_detection.py: Implements the change point detection method o base.py: Defines the base class for neural network models. o utils.py: Contains utility functions, e.g., timer and Dirichlet functions   Code for SPAMBASE (not included in the ICONIP paper draft):o ragg_model_random_noise_spambase.py: Code for Random Noise Attack o ragg_model_large_value_spambase.py: Code for Large Value Attack o ragg_label_flipping_spambase.py: Code for Label Flipping Attack   Code for MNIST:o ragg_model_random_noise_mnist.py: Code for Random Noise Attack o ragg_model_large_value_mnist.py: Code for Large Value Attack o ragg_label_flipping_minst.py: Code for Label Flipping Attack   Code for SENTIMENT140:o ragg_model_random_noise_sentiment140.py: Code for Random Noise Attack o ragg_model_large_value_sentiment140.py: Code for Large Value Attack o ragg_label_flipping_sentiment140.py: Code for Label Flipping Attack The conda environment created for this project:       nvflare-3.10      Python Version: 	      Python 3.10.14Required Libraries:      ICONIP_Requirements.txt Details of all libraries included in this conda environment:       ICONIP_All_Libraries.txt       Experimental Instructions:	# Change directory to ICONIP folder 	cd ICONIP# Run the following Python script to submit jobs to SuperPOD for three cases on MNIST and SENTIMENT140: Random Noise Attack, Large Outlier Attack, and Label Flipping Attack.      module load conda      conda activate nvflare-3.10      # This is a lightweight script for submitting jobs using sbatch	python3 sbatch_all.py # Run the following code to extract results from the log files and save them to an Excel (.xlsx) file. Before running the code, the JOBID should be replaced, which can be found in the log file name. python plot_robust_aggregation_ICONIP.py 	3. DROPOUTThis project implements three client dropout cases on SENTIMENT140: Random Client Dropout, Bernoulli Client Dropout, and Markov Client Dropout. Each case compared three aggregation methods: Refined Krum (rKrum), Krum, and Mean. The project mainly includes: * ragg folder : o robust_aggregation.py: Implements different aggregation methods, including Mean, Krum, and the Refined Krum (named adaptive_krum in the code)o change_point_detection.py: Implements the change point detection method o base.py: Defines the base class for neural network models. o utils.py: Contains utility functions, e.g., timer and Dirichlet functions * data folder : o Sentiment140: public datasetCode: o server.py: Code for servero client.py: Code for client o gen_data.py: Code for data generation for each client o model.py: Code for the neural network modelThe conda environment created for this project:       flnlp_3.9.21      Python Version: 	      Python 3.9.21Required Libraries:      DROPOUT_Requirements.txt Details of all libraries included in this conda environment:       DROPOUT_All_Libraries.txt       Experimental Instructions:	# Change directory to DROPOUT folder 	cd DROPOUT/rKrum_dropout# Submit jobs to SuperPOD for three client dropout cases on SENTIMENT140: Random Client Dropout, Bernoulli Client Dropout, and Markov Client Dropout. All the experiments were conducted on one compute node. Before submitting the job, first set the client dropout method, which can be done in server.py. For example, dropout_type ='random', which means the client dropout method is Random Client Dropout.	sbatch rkrum_20_30_1_1.sh			# Check “out/server.txt” or “out/client_ID.txt” for error or log information.	# Once the job finishes, a dropout_type.pkl can be found in the “out” folder, which stores all the experimental results. For example, if dropout_type='random', random.pkl can be found in the “out” folder. Then check dropout type in the following script, run it to extract results from the pkl file, and save them to an Excel (.xlsx) file.  python plot_dropout.py 